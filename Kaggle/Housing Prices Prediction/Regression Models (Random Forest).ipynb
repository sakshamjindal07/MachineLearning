{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After exploring the data, we're going to find of much of it can be relevant for our decision tree. This is a critical point for every Data Science project, since too much train data can easily result in bad model generalisation (accuracy on test/real/unseen observations). \n",
    "\n",
    "In this Ipython notebook, I am going to cover Random Forest. Random Forest is a versatile machine learning method capable of performing both regression and classification tasks. It also undertakes dimensional reduction methods, treats missing values, outlier values and other essential steps of data exploration, and does a fairly good job. It is a type of ensemble learning method, where a group of weak models combine to form a powerful model.\n",
    "\n",
    "Advantages of Random Forest:\n",
    "\n",
    "1) This algorithm can solve both type of problems i.e. classification and regression and does a decent estimation at both fronts.\n",
    "      \n",
    "2) One of benefits of Random forest which excites me most is, <B> the power of handle large data set with higher dimensionality. It can handle thousands of input variables and identify most significant variables so it is considered as one of the dimensionality reduction methods. </B>  Further, the model outputs Importance of variable, which can be a very handy feature (on some random data set).\n",
    "\n",
    "3) It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.\n",
    "\n",
    "4) It has methods for balancing errors in data sets where classes are imbalanced.\n",
    "\n",
    "5) The capabilities of the above can be extended to unlabeled data, leading to unsupervised clustering, data views and outlier detection.\n",
    "\n",
    "6) Random Forest involves sampling of the input data with replacement called as bootstrap sampling. Here one third of the data is not used for training and can be used to testing. These are called the out of bag samples. Error estimated on these out of bag samples is known as out of bag error. Study of error estimates by Out of bag, gives evidence to show that the out-of-bag estimate is as accurate as using a test set of the same size as the training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Functional_Min2</th>\n",
       "      <th>BldgType_Twnhs</th>\n",
       "      <th>RoofStyle_Mansard</th>\n",
       "      <th>RoofMatl_CompShg</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>GarageCond_Ex</th>\n",
       "      <th>Functional_Maj2</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>GarageType_BuiltIn</th>\n",
       "      <th>Exterior2nd_CBlock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.044522</td>\n",
       "      <td>80</td>\n",
       "      <td>9.360741</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.150603</td>\n",
       "      <td>4.976734</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.044522</td>\n",
       "      <td>81</td>\n",
       "      <td>9.565775</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>4.691348</td>\n",
       "      <td>6.828712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.110874</td>\n",
       "      <td>74</td>\n",
       "      <td>9.534668</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.674561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.110874</td>\n",
       "      <td>78</td>\n",
       "      <td>9.208238</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>6.401917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.795791</td>\n",
       "      <td>43</td>\n",
       "      <td>8.518392</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.575949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 254 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0    3.044522           80  9.360741            5            6       1961   \n",
       "1    3.044522           81  9.565775            6            6       1958   \n",
       "2    4.110874           74  9.534668            5            5       1997   \n",
       "3    4.110874           78  9.208238            6            6       1998   \n",
       "4    4.795791           43  8.518392            8            5       1992   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2         ...          \\\n",
       "0          1961    0.000000    6.150603    4.976734         ...           \n",
       "1          1958    4.691348    6.828712    0.000000         ...           \n",
       "2          1998    0.000000    6.674561    0.000000         ...           \n",
       "3          1998    3.044522    6.401917    0.000000         ...           \n",
       "4          1992    0.000000    5.575949    0.000000         ...           \n",
       "\n",
       "   Functional_Min2  BldgType_Twnhs  RoofStyle_Mansard  RoofMatl_CompShg  \\\n",
       "0                0               0                  0                 1   \n",
       "1                0               0                  0                 1   \n",
       "2                0               0                  0                 1   \n",
       "3                0               0                  0                 1   \n",
       "4                0               0                  0                 1   \n",
       "\n",
       "   SaleCondition_Partial  GarageCond_Ex  Functional_Maj2  SaleType_New  \\\n",
       "0                      0              0                0             0   \n",
       "1                      0              0                0             0   \n",
       "2                      0              0                0             0   \n",
       "3                      0              0                0             0   \n",
       "4                      0              0                0             0   \n",
       "\n",
       "   GarageType_BuiltIn  Exterior2nd_CBlock  \n",
       "0                   0                   0  \n",
       "1                   0                   0  \n",
       "2                   0                   0  \n",
       "3                   0                   0  \n",
       "4                   0                   0  \n",
       "\n",
       "[5 rows x 254 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_cleaned.csv')\n",
    "test_df  = pd.read_csv('test_cleaned.csv')\n",
    "\n",
    "id = test_df['Id']\n",
    "train_df.drop('Id',axis = 1, inplace = True)\n",
    "test_df.drop('Id',axis = 1 , inplace = True)\n",
    "y_train = train_df['SalePrice']\n",
    "x_train = train_df.drop('SalePrice', axis = 1)\n",
    "\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()\n",
    "feat_labels = train_df.columns.values\n",
    "#feat_labels\n",
    "x_train.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, Random Forest is good regression method which has an ability to handle a large amount of high-dimensional data ( here we have ~ 280 columns as features), we are going to construct a function which handles the features and list the features which are important in prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def printFeature(model,x_train, y_train , performCV = True , cvFolds = 10):\n",
    "    \n",
    "    model.fit(x_train,y_train)\n",
    "    feature_importances = model.feature_importances_\n",
    "    feat_labels = [ x for x in x_train.columns.values]\n",
    "    feat_imp = pd.Series(feature_importances,feat_labels).sort_values(ascending=False)\n",
    "    #print(feat_imp.dtypes)\n",
    "    #plt.bar(feat_labels[:],feat_imp[:])\n",
    "    plt.ylabel('Feature Importances')\n",
    "    #plt.show()\n",
    "    \n",
    "def modelFit(model,x_train,x_test,performCV = True , cvFolds = 10):\n",
    "    \n",
    "    model.fit(x_train,y_train)\n",
    "    x_train_predictions = model.predict(x_train)\n",
    "    \n",
    "    if performCV == True :\n",
    "        cv_score_meanSqError = cross_val_score(model,x_train,y_train,cv= cvFolds,scoring = 'mean_squared_error')\n",
    "        cv_score_r2 = cross_val_score(model,x_train,y_train,cv= cvFolds,scoring = 'r2_score')\n",
    "        \n",
    "    print(\"\\n Model Report\")\n",
    "    print(\"\\n Mean Squared Error = %.4g\" % metrics.mean_squared_error(x_train.values,x_train_predictions))\n",
    "    print(\"\\n R2 score = %.4g\" % metrics.r2_score(x_train.values,x_train_prediction))\n",
    "          \n",
    "    \n",
    "    print(\"CV Score (Mean Square Error) : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score_meanSqError),np.std(cv_score_meanSqError),np.min(cv_score_meanSqError),np.max(cv_score_meanSqError)))\n",
    "    print(\"CV Score ( R square        ): Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score_r2),np.std(cv_score_r2),np.min(cv_score_r2),np.max(cv_score_r2)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#defining the model\n",
    "n_estimators = 1000\n",
    "random_state = 0\n",
    "n_jobs = -1\n",
    "rf = RandomForestRegressor(n_estimators = n_estimators,random_state = random_state,n_jobs = n_jobs) \n",
    "printFeature(rf,x_train,y_train)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "modelFit(rf,x_train,y_train,performCV = True, cvFolds = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> Cross Validation </B>  is a model validation technique that splits the training dataset in a given number of \"folds\". Each split uses different data for training and testing purposes, allowing the model to be trained and tested with different data each time. This allows the algorithm to be trained and tested with all available data across all folds, avoiding any splitting bias and giving a good idea of the generalisation of the chosen model. The main downside is that Cross Validation requires the model to be trained for each fold, so the computational cost can be very high for complex models or huge datasets.\n",
    "\n",
    "Here, I am going to use K-Fold Cross Validation Technique. I am goinfg to construct a function which performa a kfold crsoo validation technique on training data set and gives out a mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
